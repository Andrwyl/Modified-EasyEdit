{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Running KN on our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to correct working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/h/120/andrewliu/EasyEdit\n"
     ]
    }
   ],
   "source": [
    "%cd /h/120/andrewliu/EasyEdit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyeditor import BaseEditor\n",
    "from easyeditor import KNHyperParams\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data\n",
    "\n",
    "set dataset variable to the correct .jsonl prefix you would like to load in. The data should be stored in EasyEditor/probing-project/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"prompts = []\\n\\nground_truth = []\\n\\ntarget_new = []\\n\\n\\nwith open('probing-project/data/' + dataset + '.jsonl', 'r') as to_read:\\n    json_list = list(to_read)\\n    for instance in json_list[0:250]:\\n        sentence = json.loads(instance)\\n        if (dataset == 'determiner_noun_agreement_1' or dataset == 'determiner_noun_agreement_with_adj_irregular_1' or dataset == 'determiner_noun_agreement_irregular_1' or dataset == 'principle_A_domain_2'):\\n            #determiner noun\\n\\n            prompts.append(sentence['one_prefix_prefix'])\\n            ground_truth.append(sentence['one_prefix_word_good'])\\n            target_new.append(sentence['one_prefix_word_bad'])\\n\\n        elif (dataset == 'regular_plural_subject_verb_agreement_1' or dataset == 'npi_present_1' or dataset == 'principle_A_case_1' or dataset == 'principle_A_case_2'):\\n            #subject verb\\n\\n\\n            prompts.append(sentence['one_prefix_prefix'])\\n            ground_truth.append(sentence['one_prefix_word_good'])\\n            target_new.append(sentence['one_prefix_word_bad'])\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'determiner_noun_agreement_1'\n",
    "\n",
    "'''prompts = ['Ray Charles, the',\n",
    "            'Grant Hill is a professional',\n",
    "            'The law in Ikaalinen declares the language'\n",
    "            ]\n",
    "ground_truth = ['piano',\n",
    "                'basketball',\n",
    "                'Finnish'\n",
    "                ]\n",
    "target_new = ['violin',\n",
    "              'soccer',\n",
    "              'Swedish'\n",
    "              ]\n",
    "subject = ['Ray Charles',\n",
    "            'Grant Hill',\n",
    "            'Ikaalinen'\n",
    "            ]'''\n",
    "prompts = []\n",
    "\n",
    "ground_truth = []\n",
    "\n",
    "target_new = []\n",
    "\n",
    "\n",
    "with open('probing-project/data/' + dataset + '.jsonl', 'r') as to_read:\n",
    "    json_list = list(to_read)\n",
    "    for instance in json_list[0:250]:\n",
    "        sentence = json.loads(instance)\n",
    "        if (dataset == 'determiner_noun_agreement_1' or dataset == 'determiner_noun_agreement_with_adj_irregular_1' or dataset == 'determiner_noun_agreement_irregular_1' or dataset == 'principle_A_domain_2'):\n",
    "            #determiner noun\n",
    "\n",
    "            prompts.append(sentence['one_prefix_prefix'])\n",
    "            ground_truth.append(sentence['one_prefix_word_good'])\n",
    "            target_new.append(sentence['one_prefix_word_bad'])\n",
    "\n",
    "        elif (dataset == 'regular_plural_subject_verb_agreement_1' or dataset == 'npi_present_1' or dataset == 'principle_A_case_1' or dataset == 'principle_A_case_2'):\n",
    "            #subject verb\n",
    "\n",
    "\n",
    "            prompts.append(sentence['one_prefix_prefix'])\n",
    "            ground_truth.append(sentence['one_prefix_word_good'])\n",
    "            target_new.append(sentence['one_prefix_word_bad'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 10:28:55,282 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "07/26/2023 10:28:55 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/u/andrewliu/EasyEdit/hugging-cache/gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 10:28:57,732 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "07/26/2023 10:28:57 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to left...\n"
     ]
    }
   ],
   "source": [
    "hparams = KNHyperParams.from_hparams('./hparams/KN/gpt2.yaml')\n",
    "\n",
    "editor = BaseEditor.from_hparams(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNHyperParams(lr_scale=1.0, n_toks=10, model_name='/u/andrewliu/EasyEdit/hugging-cache/gpt2', refine=True, batch_size=10, steps=20, adaptive_threshold=0.2, p=0.7, device=1, alg_name='KN', max_length=30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': ['Ray Charles, the', 'Grant Hill is a professional', 'The law in Ikaalinen declares the language'], 'ground_truth': ['piano', 'basketball', 'Finnish'], 'target_new': ['violin', 'soccer', 'Swedish']} [{'prompt': ['Ray Charles, the', 'Grant Hill is a professional', 'The law in Ikaalinen declares the language'], 'ground_truth': ['piano', 'basketball', 'Finnish'], 'target_new': ['violin', 'soccer', 'Swedish']}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...:  33%|███▎      | 1/3 [00:17<00:35, 17.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting attribution scores: 17.579512357711792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...:  67%|██████▋   | 2/3 [00:34<00:17, 17.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting attribution scores: 17.121166706085205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting coarse neurons for each prompt...: 100%|██████████| 3/3 [00:52<00:00, 17.55s/it]\n",
      "2023-07-26 10:29:55,991 - easyeditor.editors.editor - INFO - Execution 0 editing took 52.65162754058838\n",
      "07/26/2023 10:29:55 - INFO - easyeditor.editors.editor -   Execution 0 editing took 52.65162754058838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting attribution scores: 17.87345814704895\n",
      "\n",
      "111 coarse neurons found - refining\n",
      "3 neurons remaining after refining\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " GPT2LMHeadModel(\n",
       "   (transformer): GPT2Model(\n",
       "     (wte): Embedding(50257, 768)\n",
       "     (wpe): Embedding(1024, 768)\n",
       "     (drop): Dropout(p=0.1, inplace=False)\n",
       "     (h): ModuleList(\n",
       "       (0): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (2): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (4): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (5): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (6): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (7): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (8): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (9): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (10): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (11): GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2Attention(\n",
       "           (c_attn): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D()\n",
       "           (c_proj): Conv1D()\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       " ),\n",
       " None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor.edit(\n",
    "    prompts=prompts,\n",
    "    ground_truth=ground_truth,\n",
    "    target_new=target_new,\n",
    "    keep_original_weight=False,\n",
    "    name = dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EasyEdit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
